{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-188KMdY68Ps"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import requests\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType, FloatType, DecimalType\n",
    "\n",
    "from pyspark.sql.functions import col, when, mean, stddev, count, avg, sum, monotonically_increasing_id\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from decimal import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OosOGwe8-fQq"
   },
   "source": [
    "###Inicializando SparkSeesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8UrB0LLC-eAN"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Inicializando SparkSession\u001b[39;00m\n\u001b[0;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQualidade_de_dados\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql:postgresql:42.7.4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 6\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\java_gateway.py:104\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 104\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Inicializando SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "      .appName(\"Qualidade_de_dados\") \\\n",
    "      .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.4\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o2SuaShCL1t"
   },
   "source": [
    "###Função para extrair dados da api fornecida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABYwXF0n_1JI"
   },
   "outputs": [],
   "source": [
    "def extract_data(url_api, headers=None, params=None):\n",
    "  try:\n",
    "    response = requests.get(url_api,headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    print(f\"Response:{response.json()}\")\n",
    "    return response.text\n",
    "  except requests.exceptions.RequestException as e:\n",
    "    print(f\"Erro ao acessar API: {e}\")\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7Wwjq7KHjXt"
   },
   "outputs": [],
   "source": [
    "def define_schema():\n",
    "\n",
    "  rating = StructType([\n",
    "      StructField(\"rate\", DecimalType(10,2), True),\n",
    "      StructField(\"count\", IntegerType(), True),\n",
    "  ])\n",
    "\n",
    "  return StructType([\n",
    "      StructField(\"id\", IntegerType(), True),\n",
    "      StructField(\"title\", StringType(), True),\n",
    "      StructField(\"price\", DecimalType(10,2), True),\n",
    "      StructField(\"description\", StringType(), True),\n",
    "      StructField(\"category\", StringType(), True),\n",
    "      StructField(\"image\", StringType(), True),\n",
    "      StructField(\"rating\", rating, True)\n",
    "\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmeKQH6MCWm7"
   },
   "source": [
    "Configurar Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unEwpoSKCWPP"
   },
   "outputs": [],
   "source": [
    "url_api = \"https://fakestoreapi.com/products\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DGImcaYDXql"
   },
   "source": [
    "Configurar Banco de dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu30Zp4VAlbs"
   },
   "source": [
    "###Extrair dados da api fornecida:\n",
    "#### https://fakestoreapi.com/products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEh0maVUCGGp",
    "outputId": "bb1952da-f68a-4447-8a87-7e288056ca01"
   },
   "outputs": [],
   "source": [
    "data_api = extract_data(url_api= url_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPFQOtmrGM5n",
    "outputId": "ab1673b6-716f-4559-8b5f-0fb2ec15b6bd"
   },
   "outputs": [],
   "source": [
    "data = json.loads(data_api)\n",
    "\n",
    "#Normalizando valores para decimal\n",
    "for item in data:\n",
    "  item[\"price\"] = Decimal(str(item[\"price\"]))\n",
    "  item[\"rating\"][\"rate\"] = Decimal(str(item[\"rating\"][\"rate\"]))\n",
    "\n",
    "\n",
    "schema = define_schema()\n",
    "df = spark.createDataFrame(data, schema = schema)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFDwjmUzcNoA"
   },
   "source": [
    "### Tratamento dos valores nulos, inconsistentes e outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGrs6EBbcMkM",
    "outputId": "61f2cfc8-6aea-4a37-b8e9-0bf948788227"
   },
   "outputs": [],
   "source": [
    "# Removendo valores nulos\n",
    "df_clean = df.dropna(subset=[\"price\",\"category\",\"rating.rate\"])\n",
    "\n",
    "# Adicionar uma coluna temporária para extrair rating.rate\n",
    "df_clean = df_clean.withColumn(\"rate\", col(\"rating.rate\"))\n",
    "# Filtro de dados inconsistentes\n",
    "\n",
    "df_clean = df_clean.filter((col(\"price\") >= 0) & (col(\"rate\").between(0, 5)))\n",
    "\n",
    "\n",
    "# Removendo outliers com o método IQR (Intervalo Interquartil)\n",
    "quantiles_price = df_clean.approxQuantile(\"price\", [0.25, 0.75], 0.05)\n",
    "quantiles_rate = df_clean.approxQuantile(\"rate\", [0.25, 0.75], 0.05)\n",
    "\n",
    "# Q1, Q3 e IQR para price\n",
    "q1_price, q3_price = quantiles_price[0], quantiles_price[1]\n",
    "iqr_price = q3_price - q1_price\n",
    "lower_bound_price = q1_price - 1.5 * iqr_price\n",
    "upper_bound_price = q3_price + 1.5 * iqr_price\n",
    "\n",
    "#Q1,Q3 e IQR para rate\n",
    "q1_rate, q3_rate = quantiles_rate[0], quantiles_rate[1]\n",
    "iqr_rate = q3_rate - q1_rate\n",
    "lower_bound_rate = q1_rate - 1.5 * iqr_rate\n",
    "upper_bound_rate = q3_rate + 1.5 * iqr_rate\n",
    "\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"price\") >= lower_bound_price) & (col(\"price\") <= upper_bound_price) &\n",
    "    (col(\"rate\") >= lower_bound_rate) & (col(\"rate\") <= upper_bound_rate)\n",
    ")\n",
    "\n",
    "#Filtros para preco >= 100 e avaliação >= 3.5\n",
    "df_clean = df_clean.filter((col(\"price\") >= 100) & (col(\"rate\") >= 3.5))\n",
    "\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m33USXXXx8hT"
   },
   "source": [
    "### Sumarizaçao por categoria, preço medio, avaliação media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6kKuinySqnEm",
    "outputId": "f1bbe823-3f97-4009-b831-696afe2a2211"
   },
   "outputs": [],
   "source": [
    "df_summary = df_clean.groupBy(\"category\" ).agg(\n",
    "    avg(\"price\").alias(\"preco_medio\"),\n",
    "    avg(\"rate\").alias(\"avaliacao_media\")\n",
    ").withColumnRenamed(\"category\", \"categoria\")\n",
    "\n",
    "df_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgOIKXpdDadL"
   },
   "outputs": [],
   "source": [
    "# Configurações do PostgreSQL\n",
    "db_properties = {\n",
    "    \"url\": \"jdbc:postgresql://localhost:5432/desafio_tecnico\",  # Substitua por sua URL\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"desafio_tecnico_user\",  # Substitua pelo seu usuário\n",
    "    \"password\": \"desafiotecnico123\",  # Substitua pela sua senha\n",
    "    \"dbtable_produtos\": \"produtos\",\n",
    "    \"dbtable_summary\": \"categoria_media\"\n",
    "}\n",
    "\n",
    "db_config = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"desafio_tecnico\",  # Substitua pelo seu banco\n",
    "    \"user\": \"desafio_tecnico_user\",  # Substitua pelo seu usuário\n",
    "    \"password\": \"desafiotecnico123\"  # Substitua pela sua senha\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1magXHO7EPs"
   },
   "source": [
    "### Função para salvar Dataframe no PostreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ik6Nd_C6_Sy"
   },
   "outputs": [],
   "source": [
    "def save_to_postgres(df, table_name, db_properties, mode=\"append\"):\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_properties[\"url\"]) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "        .option(\"user\", db_properties[\"user\"]) \\\n",
    "        .option(\"password\", db_properties[\"password\"]) \\\n",
    "        .mode(mode) \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swnlPN2s7PgJ"
   },
   "source": [
    "Dividir df_clean e salvar em tabela 'produtos' com transação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUo8w1g17Nf2"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Estabelecer conexão com psycopg2 para controle transacional\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_config[\"host\"],\n",
    "        port=db_config[\"port\"],\n",
    "        database=db_config[\"database\"],\n",
    "        user=db_config[\"user\"],\n",
    "        password=db_config[\"password\"]\n",
    "    )\n",
    "    conn.autocommit = False  # Desativar commit automático\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS produtos (\n",
    "            id INTEGER,\n",
    "            title TEXT,\n",
    "            price FLOAT,\n",
    "            description TEXT,\n",
    "            category TEXT,\n",
    "            image TEXT,\n",
    "            rating_rate FLOAT,\n",
    "            rating_count INTEGER\n",
    "        );\n",
    "    \"\"\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS categoria_media (\n",
    "            category TEXT,\n",
    "            mean_price FLOAT,\n",
    "            mean_rate FLOAT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    # Partes menores de 5 registros para facilitar o processamento e a inserção no banco de dados.\n",
    "    df_clean_with_index = df_clean.withColumn(\"row_index\", monotonically_increasing_id())\n",
    "    total_rows = df_clean_with_index.count()\n",
    "    batch_size = 5\n",
    "    num_batches = (total_rows + batch_size - 1)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        df_batch = df_clean_with_index.filter(\n",
    "            (col(\"row_index\") >= start_index) & (col(\"row_index\") < end_index)\n",
    "        ).drop(\"row_index\")\n",
    "\n",
    "        # Ajustar estrutura para corresponder à tabela (desaninhar rating)\n",
    "        df_batch = df_batch.select(\n",
    "            col(\"id\"),\n",
    "            col(\"title\"),\n",
    "            col(\"price\"),\n",
    "            col(\"description\"),\n",
    "            col(\"category\"),\n",
    "            col(\"image\"),\n",
    "            col(\"rating.rate\").alias(\"rate\"),\n",
    "            col(\"rating.count\").alias(\"count\")\n",
    "        )\n",
    "\n",
    "        print(f\"Salvando batch {i+1} com {df_batch.count()} registros na tabela 'produtos'...\")\n",
    "        save_to_postgres(df_batch, db_properties[\"dbtable_produtos\"], db_properties, mode=\"append\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Salvando df_summary na tabela 'categoria_media'...\")\n",
    "    save_to_postgres(df_summary, db_properties[\"dbtable_summary\"], db_properties, mode=\"overwrite\")\n",
    "\n",
    "    conn.commit()\n",
    "    print(\"Transação confirmada com sucesso.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro durante a persistência: {str(e)}\")\n",
    "    if 'conn' in locals():\n",
    "        conn.rollback()\n",
    "        print(\"Transação revertida devido a erro.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    if 'cursor' in locals():\n",
    "        cursor.close()\n",
    "    if 'conn' in locals():\n",
    "        conn.close()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
